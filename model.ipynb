{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk5yCDr9jLGQ"
      },
      "outputs": [],
      "source": [
        "# @title Step 1: Install the Stable Configuration\n",
        "\n",
        "!pip install sentence-transformers faiss-cpu nltk numpy spicy --quiet\n",
        "\n",
        "print(\"✅ All libraries installed.\")\n",
        "print(\"🔴 IMPORTANT: Please restart the runtime now before proceeding.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Download NLTK Data and Set Up Paths\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "# Download both the standard and specialized 'punkt' tokenizers to prevent errors\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "print(\"✅ NLTK tokenizers downloaded.\")\n",
        "\n",
        "# --- Mount Google Drive & Set Up Paths ---\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "TEXTBOOK_DIR = '/content/drive/MyDrive/Medical_Textbooks'\n",
        "SAVE_DIR = '/content/drive/MyDrive/medprompt_ai_data'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nTextbooks will be loaded from: {TEXTBOOK_DIR}\")\n",
        "print(f\"Output files will be saved to: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "q7zlXtvKjVCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Process Textbooks and Generate Data Files (High-Efficiency GPU Method)\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "# --- 1. Verify GPU Availability ---\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(f\"✅ GPU is available. Using device: {device}\")\n",
        "else:\n",
        "    # Fallback to CPU if no GPU is found, though it will be much slower.\n",
        "    device = 'cpu'\n",
        "    print(\"⚠️ WARNING: GPU not found. Falling back to CPU. Processing will be significantly slower.\")\n",
        "    print(\"To fix this, go to Runtime > Change runtime type and select 'T4 GPU'.\")\n",
        "\n",
        "\n",
        "# ======================================================================================\n",
        "# --- ⚙️ HYPERPARAMETER CONFIGURATION ---\n",
        "# ======================================================================================\n",
        "\n",
        "# --- Model Selection ---\n",
        "# BAAI/bge-base-en-v1.5 is a powerful model that runs efficiently on a T4 GPU.\n",
        "LOCAL_EMBEDDING_MODEL = 'BAAI/bge-base-en-v1.5'\n",
        "\n",
        "# --- Chunking Strategy ---\n",
        "MIN_PARAGRAPH_LEN = 40       # Skips paragraphs shorter than this character length.\n",
        "MIN_CHUNK_LEN = 30           # Skips final chunks shorter than this character length.\n",
        "OVERLAP_SENTENCES = 1        # Number of previous sentences to include for context.\n",
        "\n",
        "# --- FAISS Index Tuning ---\n",
        "# Set to True for very large datasets (>100k chunks) to potentially speed up retrieval.\n",
        "USE_ADVANCED_FAISS_INDEX = True\n",
        "FAISS_NLIST = 100            # Number of cells for the advanced IVF index.\n",
        "# ======================================================================================\n",
        "\n",
        "\n",
        "# --- NLTK Setup ---\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def robust_sentence_splitter(paragraph):\n",
        "    \"\"\"Splits a paragraph into sentences using NLTK.\"\"\"\n",
        "    return sent_tokenize(paragraph)\n",
        "\n",
        "def process_textbooks():\n",
        "    \"\"\"\n",
        "    Loads text files, processes them, generates embeddings locally on the GPU,\n",
        "    builds a FAISS index, and saves the results.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(TEXTBOOK_DIR):\n",
        "        print(f\"Error: The directory '{TEXTBOOK_DIR}' was not found.\")\n",
        "        return\n",
        "\n",
        "    # --- Dynamic File Discovery ---\n",
        "    textbook_filenames = [f for f in os.listdir(TEXTBOOK_DIR) if f.endswith('.txt')]\n",
        "    print(f\"\\nFound {len(textbook_filenames)} textbooks to process.\")\n",
        "\n",
        "    # --- Process Textbooks into Chunks ---\n",
        "    print(\"\\n--- Processing Textbooks and Generating Chunks ---\")\n",
        "    text_chunks = []\n",
        "    # (This logic remains the same as your provided code)\n",
        "    for filename in tqdm(textbook_filenames, desc=\"Processing Textbooks\"):\n",
        "        filepath = os.path.join(TEXTBOOK_DIR, filename)\n",
        "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        paragraphs = content.split('\\n\\n')\n",
        "        for para in paragraphs:\n",
        "            if len(para.strip()) < MIN_PARAGRAPH_LEN:\n",
        "                continue\n",
        "\n",
        "            sentences = robust_sentence_splitter(para)\n",
        "            if not sentences:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(sentences)):\n",
        "                chunk_content = sentences[i]\n",
        "                start_index = max(0, i - OVERLAP_SENTENCES)\n",
        "                contextual_prefix = \" \".join(sentences[start_index:i])\n",
        "                full_chunk = (contextual_prefix + \" \" + chunk_content).strip()\n",
        "\n",
        "                if len(full_chunk) > MIN_CHUNK_LEN:\n",
        "                    text_chunks.append({\"text\": full_chunk, \"source\": filename})\n",
        "\n",
        "    print(f\"\\nTotal text chunks created: {len(text_chunks)}\")\n",
        "    if not text_chunks:\n",
        "        return\n",
        "\n",
        "    # --- Generate Embeddings Locally on GPU ---\n",
        "    print(f\"\\n--- Loading local model '{LOCAL_EMBEDDING_MODEL}' onto the {device.upper()} ---\")\n",
        "    embedding_model = SentenceTransformer(LOCAL_EMBEDDING_MODEL, device=device)\n",
        "\n",
        "    print(\"\\n--- Generating Embeddings ---\")\n",
        "    chunk_texts = [chunk['text'] for chunk in text_chunks]\n",
        "\n",
        "    # The 'encode' method is highly optimized for batch processing on a GPU.\n",
        "    # A large batch size here is good for performance.\n",
        "    chunk_embeddings = embedding_model.encode(\n",
        "        chunk_texts,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=128\n",
        "    )\n",
        "\n",
        "    embedding_dimension = chunk_embeddings.shape[1]\n",
        "    print(f\"Generated {len(chunk_embeddings)} embeddings of dimension {embedding_dimension}.\")\n",
        "\n",
        "    # --- Build and Save FAISS Index ---\n",
        "    print(\"\\n--- Building and Saving FAISS Index ---\")\n",
        "    if USE_ADVANCED_FAISS_INDEX:\n",
        "        print(f\"Using advanced FAISS index 'IndexIVFFlat' with nlist={FAISS_NLIST}.\")\n",
        "        quantizer = faiss.IndexFlatL2(embedding_dimension)\n",
        "        index = faiss.IndexIVFFlat(quantizer, embedding_dimension, FAISS_NLIST)\n",
        "        print(\"Training the advanced index...\")\n",
        "        index.train(chunk_embeddings)\n",
        "    else:\n",
        "        print(\"Using standard FAISS index 'IndexFlatL2'.\")\n",
        "        index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "    index.add(chunk_embeddings.astype('float32'))\n",
        "    print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
        "\n",
        "    # --- Save Files ---\n",
        "    faiss_index_file_path = os.path.join(SAVE_DIR, 'medical_faiss_index.bin')\n",
        "    faiss.write_index(index, faiss_index_file_path)\n",
        "    print(f\"FAISS index saved to: {faiss_index_file_path}\")\n",
        "\n",
        "    chunks_file_path = os.path.join(SAVE_DIR, 'medical_text_chunks.pkl')\n",
        "    with open(chunks_file_path, 'wb') as f:\n",
        "        pickle.dump(text_chunks, f)\n",
        "    print(f\"Text chunks saved to: {chunks_file_path}\")\n",
        "\n",
        "    print(\"\\n--- ✅ Pre-processing Complete ---\")\n",
        "\n",
        "# --- Run the main function ---\n",
        "if __name__ == '__main__':\n",
        "    process_textbooks()"
      ],
      "metadata": {
        "id": "lEfiyvBojZ0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}